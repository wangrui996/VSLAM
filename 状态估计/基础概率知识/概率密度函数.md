# 基础概率知识——概率密度函数  


## 概率密度函数  

**概率密度函数的定义**： x为区间[a,b]上的随机变量，服从某个概率密度函数p(x)，则这个非负函数必须满足：    
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146008025-9b91c181-8f6c-4c38-8ec4-43e13454577c.png"></p>  

* 概率密度函数在某个区间上积分得到概率：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146008431-c9990980-53e9-40fe-88a6-e84559448a45.png"></p>  

* 如果x表达某种状态，也称为x在该区间下的可能性/似然 ：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146010739-a4577afb-2f8c-492e-b046-cf8101695436.png"></p>  

 
## 条件概率  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146011860-c026abd0-0617-4024-a35b-4e19c4d5ad60.png"></p>   
 
有时候可能比较绕，因为x可能是y的函数或者y是x的函数，或者两者没有直接关系？
 
 
## 联合概率  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146012241-808907c8-d459-41d3-a8fa-945189a4b634.png"></p>  
 
多个随机变量  
 
 * 联合概率也是满足全概率公理的：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146013133-f81071a7-7c70-43b6-8fe5-934cdeb063cd.png"></p> 
联合概率密度函数在各自能取到区间上N重积分也是1  



## 贝叶斯公式  

**贝叶斯公式： 联合概率 = 条件 * 边缘**  

<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146014686-636c0ead-31bb-4109-9a28-8b1e203c5b19.png"></p>  
上面式子转换后就得到了经典的贝叶斯公式形式：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146014751-d7ec3dfd-e725-463d-a8d5-eedfc3592f77.png"></p>  

结合到实际物理意义，可以这么认为：x=状态, y=传感器读数（测量, p(y|x)=传感器模型, p(x|y)=状态估计  
我们想要知道的是在y这种测量下状态x的分布的形式  

这里的p(x)一般叫做先验，p(y|x)叫做似然  

可以看到：
* 似然p(y|x)通常容易得到，因为我们是有传感器模型的
* p(x)有些情况下可能会知道  
* p(y)通常没法直接计算，它的物理意义是，传感器读数的分布，比如一张图像有猫的概率是多大？，因为像素有很多，这个概率很难说，而且它的分布就更难说)  
* 实用中不会直接描述p(x|y)的分布(比较难得到)，通常会取某些近似的形式（如高斯）  


## 矩  

当一个分布比较复杂，难以描述时怎么办，通常会用一些简单的性质或者简单的参数刻画这个分布，比较简单的方式是计算这个分布的矩  

### 矩  
* 0阶矩恒等于1  
* 1阶矩称为期望（Expectation）：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146018168-d002aff0-db22-4457-bdf6-d23e3d57e975.png"></p>  

* 2阶矩称为协方差(Covariance):  

<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146018796-5ba8010c-b05a-448b-8c3c-bafad484c849.png"></p>  

* 3阶矩和4阶矩称为偏度和峰度  

为什么我们经常用假设一个分布是高斯分布？  
* 对于一个复杂的分布，如果把它的高阶矩舍掉，只保留1和2阶矩去刻画（这是对于任何分布都可以去算出来的）,相当于用一个高斯分布去近似。  
* 如何估计某个随机变量的矩？ 假设一个随机变量不知道它什么样子，只有它的样本————通过样本去计算
* 每次传感器测量数据就是一个变量的样本(measurement)  
* 如果有很多样本，就可以计算该随机变量的1和2阶矩：  

<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146020474-26c122a3-7079-467e-8efd-63f38212da8e.png"></p>  

<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146020718-510ff1f6-6123-4273-a3ba-f1313930b598.png"></p>  

这里的N-1称为[贝塞尔修正]()  


## 随机变量之间的关系  

* 随机变量的统计独立性  
如果两个随机变量x和y的联合概率密度可以用以下因式分解，那么我们称这两个随机变量是**统计独立**的： 即联合概率密度是各自概率密度的乘积  

<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146021626-ed69cbd3-740a-40f5-978d-c4a717cf30b1.png"></p>  

* 不相关性：
如果两个随机变量的期望满足下式，则是**不相关的**：  
<p align="center"><img src="https://user-images.githubusercontent.com/58176267/146022514-e650b509-e26e-4bf8-bbb7-e80f02b46e5c.png"></p>  

* 独立一定是不相关的，反之没法推出来  
* 对于高斯分布：独立=不相关 是可以相互推出来的  



 


 

 


 





